{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Classical/Frequentist Statistical Inference: I\n",
    "\n",
    "*J. S. Hazboun (2024)*\n",
    "\n",
    "Material in this lecture and notebook is based upon Stephen Taylor's Astrostatistics clas at Vanderbilt, which are in turn based on the \"Maximum Likelihood and Applications in Astronomy\" lectures of A. Connolly's & Ž. Ivezić's \"Astrostatistics & Machine Learning\" class at the University of Washington (ASTR 598, https://github.com/dirac-institute/uw-astr598-w18). Also the \"Inference\" lectures of G. Richards' \"Astrostatistics\" class at Drexel University (PHYS 440/540, https://github.com/gtrichards/PHYS_440_540), J. Bovy's mini-course on \"Statistics & Inference in Astrophysics\" at the University of Toronto (http://astro.utoronto.ca/~bovy/teaching.html). \n",
    "\n",
    "\n",
    "***Exercises required for class participation are in <font color='red'>red</font>.***\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "* [Statistical Inference](#one)\n",
    "* [Maximum Likelihood Estimation](#two)\n",
    "* [MLE Applied To A Homoscedastic Gaussian](#three)\n",
    "* [Quantifying Estimate Uncertainty](#four)\n",
    "* [MLE Applied To A Heteroscedastic Gaussian](#five)\n",
    "* [Working With non-Gaussian Likelihoods](#six)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Statistical Inference <a class=\"anchor\" id=\"one\"></a>\n",
    "\n",
    "Statistical *inference* is about drawing conclusions from data, specifically determining the properties of a population by data sampling.\n",
    "\n",
    "Three examples of inference are:\n",
    "1. What is the best estimate for a model parameter?\n",
    "2. How confident are we about our result?\n",
    "3. Are the data consistent with a particular model/hypothesis?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Some Terminology\n",
    "\n",
    "* We typically study the properties of some ***population*** by measuring ***samples*** from that population. The population doesn't have to refer to different objects. E.g., we may be (re)measuring the position of an object at rest; the population is the distribution of (an infinite number of) measurements smeared by the uncertainty, and the sample are the measurement we've actually taken.\n",
    "\n",
    "\n",
    "* A ***statistic*** is any function of the sample. For example, the sample mean is a statistic. But also, \"the value of the first measurement\" is also a statistic. Don't stress too much about how to make a statistic; it's just a way of summarizing data in a way that helps reveal the presence of a signal. We will meet ways of finding the optimal statistic for a given scenario.\n",
    "\n",
    "\n",
    "* To conclude something about the population from the sample, we develop ***estimators***. An estimator is a statistic, a rule for calculating an estimate of a given quantity based on observed data.\n",
    "\n",
    "\n",
    "* There are ***point*** and ***interval estimators***. The point estimators yield single-valued results (example: the position of an object), while with an interval estimator, the result would be a range of plausible values (example: confidence interval for the position of an object).\n",
    "\n",
    "\n",
    "* Measurements have **uncertainties** (not errors) and we need to account for these (sometimes they are unknown). Data are not variables but fixed values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Frequentist vs. Bayesian Inference\n",
    "\n",
    "There are two major statistical paradigms that address the statistical inference questions: \n",
    "- the **classical**, or **frequentist** paradigm,\n",
    "- the **Bayesian** paradigm.\n",
    "\n",
    "While most of statistics and machine learning is based on the classical paradigm, Bayesian techniques are being embraced by the statistical and scientific communities at an ever-increasing pace...especially in astrophysics.\n",
    "\n",
    "#### Key differences\n",
    "- **Definition of probabilities**:\n",
    "    - In ***frequentist inference***, probabilities describe the ***relative frequency of events*** over repeated experimental trials. \n",
    "    - In ***Bayesian inference***, probabilities instead quantify our ***subjective belief about experimental outcomes, model parameters, or even models themselves***. \n",
    "    \n",
    "    \n",
    "- **Quantifying uncertainty**:\n",
    "    - In ***frequentist inference*** we have ***confidence levels*** that describe the distribution of the measured parameter from the data around the true value.\n",
    "    - In ***Bayesian inference*** we have ***credible regions*** derived from posterior probabilitiy distributions (we'll meet these later). These encode our \"***belief spread***\" in model parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example\n",
    "Let's say that you get the results of an IQ test.  Any given test result might not give you your \"real\" IQ (whatever that means anyway...).  But it gives us a way to *estimate* it (and the possible range of values).  \n",
    "- For a frequentist, the best estimator is the average of many test results.  So, if you took 5 IQ tests and got a sample mean of 160, then that would be the estimator of your true IQ.\n",
    "- On the other hand, a Bayesian would say: \"*but wait, I know that IQ tests are calibrated to have a mean of 100 with a standard deviation of 15 points*\".  So they will use that as \"prior\" information, which is important here since 160 is a 4$\\sigma$ outlier. \n",
    "\n",
    "There's nothing mysterious about priors. It simply encodes any previous knowledge or information we have about our experiment.\n",
    "\n",
    "The following article provides a nice example that is visualized below (without the detailed math) [Efron 1978](http://www.jstor.org/stable/2321163?seq=1#page_scan_tab_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this cell\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "import scipy.stats\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import uniform\n",
    "from astroML import stats as astroMLstats\n",
    "\n",
    "#from astroML.plotting import setup_text_plots\n",
    "#setup_text_plots(fontsize=10, usetex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>Complete and execute the following cell. It will show the simple average of IQ tests, the prior distribution one would use in a Bayesian analysis, and the final Bayesian posterior \"belief\" distribution.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------\n",
    "# Define the distributions to be plotted\n",
    "sigma_values = [___, 6.7, 1] #complete with the prior width of IQ distribution\n",
    "linestyles = ['--', '-', ':']\n",
    "mu_values = [____, 148, 160] #complete with the prior mean of IQ distribution\n",
    "labeltext = ['prior dist.', \n",
    "             'posterior dist.', \n",
    "             'observed mean']\n",
    "xplot = np.linspace(50, 200, 1000)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# plot the distributions\n",
    "fig, ax = plt.subplots(figsize=(10, 7.5))\n",
    "\n",
    "for sigma, ls, mu, lab in zip(sigma_values, \n",
    "                              linestyles, \n",
    "                              mu_values, \n",
    "                              labeltext):\n",
    "    \n",
    "    # create a gaussian / normal distribution\n",
    "    dist = norm(mu, sigma)\n",
    "\n",
    "    if sigma > 1:\n",
    "        plt.plot(xplot, dist.pdf(xplot), \n",
    "                 ls=ls, c='black',\n",
    "                 label=r'%s $\\mu=%i,\\ \\sigma=%.1f$' % (lab, mu, sigma))\n",
    "    else:\n",
    "        plt.plot([159.9, 160.1], [0, 0.8], \n",
    "                 ls=ls, color='k', label=r'%s $\\mu=%i$' % (lab, mu))\n",
    "        \n",
    "plt.xlim(50, 200)\n",
    "plt.ylim(0, 0.1)\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel(r'$p(x|\\mu,\\sigma)$')\n",
    "plt.title('Gaussian Distribution')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The end result (skipping over the detailed math) is that the Bayesian estimate of the IQ is not 160, but rather 148, or more specifically that $p(141.3\\le \\mu \\le 154.7 \\, | \\, \\overline{x}=160) = 0.683$. This estimate incorporates the prior information of how the IQ distribution is calibrated. We also saw in *Lecture 2* the huge influence that priors can have.\n",
    "\n",
    "This all seems totally fine; where's the controversy with Bayesian methods? The controversy arises when we don't know the prior distribution, or when the parameter is fixed but we are trying to experimentally verify it (e.g., the speed of light). We'll return to this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Maximum Likelihood Estimation (MLE) <a class=\"anchor\" id=\"two\"></a>\n",
    "\n",
    "Let's talk about maximum likelihood estimation, which is relevant to both Bayesian and Frequentist approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Maximum Likelihood Approach\n",
    "\n",
    "Maximum likelihood estimation follows this blueprint:\n",
    "\n",
    "1. **Hypothesis**: Formulate a model, a *hypothesis*, about how the data are generated. For example, the data are a measurement of some quantity with Gaussian random uncertainties (i.e., each measurement is equal to the true value, plus a deviation randomly drawn from the normal distribution). Models are typically described using a set of model parameters $\\boldsymbol{\\theta}$, and written as $\\boldsymbol{M}(\\boldsymbol{\\theta})$.\n",
    "\n",
    "\n",
    "2. **Maximum Likelihood Estimation**: Search for the \"best\" model parameters $\\boldsymbol{\\theta}$ which maximize the ***likelihood*** $L(\\boldsymbol{\\theta}) \\equiv p(D|M)$. This search yields the MLE *point estimates*, $\\boldsymbol{\\theta^0}$.\n",
    "\n",
    "\n",
    "3. **Quantifying Estimate Uncertainty**: Determine the confidence region for model parameters, $\\boldsymbol{\\theta^0}$. Such a confidence estimate can be obtained analytically (possibly with some approximations), but can also be done numerically for arbitrary models using general frequentist techniques, such as bootstrap, jackknife, and cross-validation (we'll come to these later).\n",
    "\n",
    "\n",
    "4. **Hypothesis Testing**: Perform hypothesis tests as needed to make other conclusions about models and point estimates. Possibly GOTO #1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: Measuring the Position of a Quasar\n",
    "\n",
    "Let's assume we wish to estimate the position $x$ of a quasar from a series of individual astrometric measurements.\n",
    "\n",
    "1. We adopt a model where the observed quasar does not move, and has individual measurement uncertainties \n",
    "2. We derive the expression for the likelihood of there being a quasar at position $x_0$ that gives rise to our individual measurements. We find the value of $\\hat x_0$ for which our observations are maximally likely.\n",
    "3. We determine the uncertainties (confidence intervals) on our measurement.\n",
    "4. We test whether what we've observed is consistent with our adopted model. For example, is it possible that the quasar was really a misidentified star with measurable proper motion?\n",
    "\n",
    "Note: in the text to come, I will use $\\mu$ instead of $x_0$ to denote the true position of the quasar. This is to avoid potential confusion with the first (or zeroth) measurement of $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<font color='red'>Stop here for a minute and talk with your colleagues. What likelihood function would you choose as an approximation for this situation? What underlying assumptions did you make?</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Likelihood Function\n",
    "\n",
    "If we know the distribution from which our data were drawn (or make a hypothesis about it), then we can compute the **probability** of our data being generated.\n",
    "\n",
    "For example, if our data are generated by a Gaussian process with mean $\\mu$ and standard deviation $\\sigma$, then the probability density of a certain value $x$ is\n",
    "\n",
    "$$p(x|\\mu,\\sigma) = \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\left(\\frac{-(x-\\mu)^2}{2\\sigma^2}\\right).$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Execute this cell\n",
    "#------------------------------------------------------------\n",
    "# plot the distributions\n",
    "fig, ax = plt.subplots(figsize=(10, 7.5))\n",
    "dist = norm(5, 1)\n",
    "x = np.linspace(0, 10, 1000)\n",
    "plt.plot(x, dist.pdf(x), c='black',label=r'$\\mu=5,\\ \\sigma=1$')\n",
    "\n",
    "plt.xlim(0, 10)\n",
    "plt.ylim(0, 0.5)\n",
    "\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel(r'$p(x|\\mu=5,\\sigma=1)$')\n",
    "plt.title('Probability of $x$')\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# useful to know you can do this...\n",
    "norm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If we want to know the total probability of our ***entire*** data set (as opposed to one measurement) then we must compute the ***product*** of all the individual probabilities:\n",
    "\n",
    "$$L \\equiv p(\\{x_i\\}|M(\\theta)) = \\prod_{i=1}^N p(x_i|M(\\theta)),$$\n",
    "\n",
    "where $M$ is the *model* and $\\theta$ refers collectively to the $k$ parameters of the model, which can generally be multi-dimensional. In words...\n",
    "\n",
    "> $L(\\{x_i\\})\\equiv$ the probability of the data given the model parameters. \n",
    "\n",
    "If we consider $L$ as a function of the model parameters, we refer to it as\n",
    "\n",
    "> $L(\\theta)\\equiv$ likelihood of the model parameters, given the observed data. \n",
    "\n",
    "Note:\n",
    "- [Jeynes](https://www.amazon.com/Probability-Theory-Science-T-Jaynes/dp/0521592712) is quite strict on how refer to the likelihood of model parameters versus the probability of the data, but we'll be a bit more lax.\n",
    "- while the components of $L$ may be normalized pdfs, their product is not.\n",
    "- the product can be very small, so we often take the log of $L$. \n",
    "- we're assuming the individual measurements are independent of each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can write $L$ out as\n",
    "\n",
    "$$L = \\prod_{i=1}^N \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\left(\\frac{-(x_i-\\mu)^2}{2\\sigma^2}\\right),$$\n",
    "\n",
    "and simplify to\n",
    "\n",
    "$$L = \\left( \\prod_{i=1}^N \\frac{1}{\\sigma\\sqrt{2\\pi}} \\right) \\exp\\left( -\\frac{1}{2} \\sum \\left[\\frac{-(x_i-\\mu)}{\\sigma} \\right]^2 \\right),$$\n",
    "\n",
    "where we have written the ***product of the exponentials as the exponential of the sum of the arguments***, which will make things easier to deal with later.\n",
    "\n",
    "To repeat, all we have done is this: \n",
    "\n",
    "$$\\prod_{i=1}^N A_i \\exp(-B_i) = (A_iA_{i+1}\\ldots A_N) \\exp[-(B_i+B_{i+1}+\\ldots+B_N)]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If you have done $\\chi^2$ analysis (e.g., doing a linear least-squares fit), then you might notice that the argument of the exponential is just \n",
    "\n",
    "$$\\exp \\left(-\\frac{\\chi^2}{2}\\right).$$\n",
    "\n",
    "That is, for our gaussian distribution\n",
    "\n",
    "$$\\chi^2 = \\sum_{i=1}^N \\left ( \\frac{x_i-\\mu}{\\sigma}\\right)^2.$$\n",
    "\n",
    "So, **maximizing the likelihood or log-likelihood is the same as minimizing $\\chi^2$**.  In both cases we are finding the most likely values of our model parameters (here $\\mu$ and $\\sigma$).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Key Idea Behind Maximum Likelihood Estimation\n",
    "\n",
    "Let's say that we know that some data were drawn from a Gaussian distribution, but we don't know the $\\theta = (\\mu,\\sigma)$ values of that distribution (i.e., the parameters).\n",
    "\n",
    "Then Maximum Likelihood Estimation method tells us to think of the likelihood as a ***function of the unknown model parameters***, and to ***find the parameters that maximize the value of $L$***. Those will be our *Maximum Likelihood Estimators* for for the true values of the model.\n",
    "\n",
    "Take a look at this [animation of linear least squares fitting](https://yihui.org/animation/example/least-squares/).\n",
    "\n",
    "They are trying to fit a line to some data by trying different intercepts and slopes. The red dashed lines show the difference (*residual*) between the model predicted value and the actual value. These are squared and summed ($\\chi^2$) and plotted as the $y$-axis in the right hand plot. The best-fit model parameters minimize the $\\chi^2$ value and maximize the likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## MLE applied to a Homoscedastic Gaussian <a class=\"anchor\" id=\"three\"></a>\n",
    "\n",
    "Let's take a look at our astrometry example, using a model where all the measurements have the same uncertainty, drawn from a normal distribution, $N(0, \\sigma)$.\n",
    "\n",
    "As mentioned back in our early lectures, uncertainties being the same is known as having **homoscedastic** uncertainties which just means \"uniform uncertainties\".  Later we will consider the case where the measurements can have different uncertainties ($\\sigma_i$) which is called **heteroscedastic**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We have an experiment with the set of measured positions $D=\\{x_i\\}$ in 1D with Gaussian uncertainties, and therefore:\n",
    "\n",
    "$$L \\equiv p(\\{x_i\\}|\\mu,\\sigma) = \\prod_{i=1}^N \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\left(\\frac{-(x_i-\\mu)^2}{2\\sigma^2}\\right).$$\n",
    "\n",
    "Note that that is $p(\\{x_i\\})$ not $p(x_i)$, that is the probability of the full data set, not just one measurement. If $\\sigma$ is both constant and *known*, then this is a one parameter model with $k=1$ and $\\theta_1=\\mu$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "As we found above, likelihoods can be really small, so let's define the **log-likelihood function** as ${\\ln L} = \\ln[L(\\theta)]$.  The maximum of this function happens at the same place as the maximum of $L$.  Note that any constants in $L$ have the same effect for all model parameters, so constant terms can be ignored.  \n",
    "\n",
    "In this case we then have \n",
    "\n",
    "$${\\rm lnL} = {\\rm constant} - \\sum_{i=1}^N \\frac{(x_i - \\mu)^2}{2\\sigma^2}.$$\n",
    "\n",
    "Take a second and make sure that you understand how we got there.  It might help to remember that above, we wrote\n",
    "\n",
    "$$L = \\prod_{i=1}^N \\left( \\frac{1}{\\sigma\\sqrt{2\\pi}} \\right) \\exp\\left( -\\frac{1}{2} \\sum \\left[\\frac{-(x_i-\\mu)}{\\sigma} \\right]^2 \\right).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We then determine the maximum in the same way that we always do.  It is the parameter set for which the derivative of ${\\rm lnL}$ is zero:\n",
    "\n",
    "$$\\frac{d\\;{\\rm lnL}(\\mu)}{d\\mu}\\Biggr\\rvert_{\\hat \\mu} \\equiv 0.$$\n",
    "\n",
    "That gives $$ \\sum_{i=1}^N \\frac{(x_i - \\hat \\mu)}{\\sigma^2} = 0.$$\n",
    "\n",
    "Note: \n",
    "- We should also check that the $2^{\\rm nd}$ derivative is negative, to ensure this is the *maximum* of $L$.\n",
    "- Any constants in $\\ln L$ disappear when differentiated, so constant terms can typically be ignored. This will change if we're trying to select between different models, rather than just parameter estimation within a single model as we're doing here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Since $\\sigma = {\\rm constant}$ (noy always, but here at least), that says \n",
    "\n",
    "$$\\sum_{i=1}^N x_i = \\sum_{i=1}^N \\hat \\mu = N \\hat \\mu.$$\n",
    "\n",
    "Thus we find that\n",
    "\n",
    "$$\\hat \\mu = \\frac{1}{N}\\sum_{i=1}^N x_i,$$\n",
    "\n",
    "***which is just the sample arithmetic mean of all the measurements!*** Thus **the sample mean is a ML estimator**. We got there in a roundabout way, but still pretty easy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Properties of ML Estimators\n",
    "\n",
    "Assuming the data truly are drawn from the model, ML estimators have the following useful properties:\n",
    "\n",
    "* **They are consistent estimators**. They converge to the true parameter value as $N\\to\\infty$.\n",
    "\n",
    "\n",
    "* **They are asymptotically normal estimators**. As $N\\to\\infty$ the distribution of the parameter estimate approaches a normal distribution, centered at the MLE, with a certain spread.\n",
    "\n",
    "\n",
    "* **They asymptotically achieve the theoretical minimum possible variance, called the Cramér–Rao bound**. They achieve the best possible uncertainty given the data at hand; no other estimator can do better in terms of efficiently using each data point to reduce the total error of the estimate (see eq. 3.33 in the textbook)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to draw a homoscedastic sample of ${x_i}$ from a Gaussian and compute the likelihood.\n",
    "\n",
    "<font color='red'>First generate a sample of `N=3` points drawn from a normal distribution with `mu=1.0` and `sigma=0.2`: $\\mathscr{N}(\\mu,\\sigma)$</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = ___ #Complete\n",
    "mu = ___\n",
    "sigma = ___ \n",
    "np.random.seed(42)\n",
    "sample = norm(___,___).rvs(___)\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Treat each of these observations as an estimate of the true distribution. So we'll center a Gaussian (with the known $\\sigma$) at each point. This is the probability of each data point, $p(x_i|\\mu,\\sigma)$.\n",
    "\n",
    "Plot each of the likelihoods separately.  Also plot their product. Make the $x$ axis a grid of 1000 points uniformly sampled between $x=0$ and $x=2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make the plot and see if you get the same as me.\n",
    "xgrid = np.linspace(___,___,___)\n",
    "L1 = norm.pdf(___,loc=___,scale=___) #This is a Gaussian PDF sampled uniformly, centered at a specific location.\n",
    "L2 = norm.pdf(___,loc=___,scale=___)\n",
    "L3 = norm.pdf(___,loc=___,scale=___)\n",
    "L = ___ #Total L is ???\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 5))\n",
    "plt.plot(xgrid, L1, ls='-', c='green', label=r'$L(x_1)$')\n",
    "plt.plot(xgrid, L2, ls='-', c='red', label=r'$L(x_2)$')\n",
    "plt.plot(xgrid, L3, ls='-', c='blue', label=r'$L(x_3)$')\n",
    "plt.plot(xgrid, L, ls='-', c='black', label=r'$L(\\{x\\})$')\n",
    "\n",
    "plt.xlim(0.2, 1.8)\n",
    "plt.ylim(0, 8.0)\n",
    "plt.xlabel('$\\mu$') #Leave out or adjust if no latex\n",
    "plt.ylabel(r'$p(x_i|\\mu,\\sigma)$') #Leave out or adjust if no latex\n",
    "plt.title('MLE for Gaussian Distribution')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can just read off the maximum likelihood solution.  <font color='red'>Use `np.argsort()` to figure out the index of the largest value and print that element of `xgrid`.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_indices = ___.____(___)\n",
    "index_max = ___[___]\n",
    "print(\"Likelihood is maximized at %.3f\" % ___[___])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Quantifying Estimate Uncertainty <a class=\"anchor\" id=\"four\"></a>\n",
    "\n",
    "Our ML estimate of $\\mu$ is not perfect. The uncertaintly of the estimate is captured by the shape and distribution of the likelihood function, but we'd like to capture that with a few numbers.\n",
    "\n",
    "The ***asymptotic normality of MLE*** is invoked to approximate the likelihood function as a Gaussian (or the $\\ln L$ as a parabola), i.e. we take a Taylor expansion around the MLE, keep terms up $2^\\mathrm{nd}$ order, then *define* the uncertainty on our model parameters as:\n",
    "\n",
    "$$\\sigma_{jk} = \\sqrt{[F^{-1}]_{jk}}, $$\n",
    "\n",
    "where\n",
    "\n",
    "$$ F_{jk} = - \\frac{\\partial^2}{\\partial\\theta_j} \\frac{\\ln L}{\\partial\\theta_k} \\Biggr\\rvert_{\\theta=\\hat \\theta}.$$\n",
    "\n",
    "The matrix $F$ is known as the **observed Fisher information matrix**. The elements $\\sigma^2_{jk}$ are known as the ***covariance matrix***.\n",
    "\n",
    "The marginal error bars for each parameter, $\\theta_i$ are given by the diagonal elements, $\\sigma_{ii}$. These are the \"error bars\" that are typically quoted with each measurement. Off diagonal elements, $\\sigma_{ij}$, arise from any correlation between the parameters in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In our example of a homoscedastic Gaussian, the uncertainly on the mean is \n",
    "\n",
    "$$\\sigma_{\\mu} = \\left( - \\frac{\\partial^2\\ln L(\\mu)}{\\partial\\mu^2}\\Biggr\\rvert_{\\hat \\mu}\\right)^{-1/2}$$\n",
    "\n",
    "We find\n",
    "\n",
    "$$\\frac{\\partial^2\\ln L(\\mu)}{\\partial\\mu^2}\\Biggr\\rvert_{\\hat \\mu} = - \\sum_{i=1}^N\\frac{1}{\\sigma^2} = -\\frac{N}{\\sigma^2},$$\n",
    "\n",
    "since, again, $\\sigma = {\\rm constant}$.  \n",
    "\n",
    "Then \n",
    "\n",
    "$$\\sigma_{\\mu} = \\frac{\\sigma}{\\sqrt{N}}.$$\n",
    "\n",
    "So, our estimator of $\\mu$ is $\\overline{x}\\pm\\frac{\\sigma}{\\sqrt{N}}$, which is a result that you should be familiar with. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's check this with a quick and dirty calculation. In the following, we \n",
    "- do a rough $2^\\mathrm{nd}$ order differentation of our log-likelihood function with `np.diff`, \n",
    "- divide through by our $\\Delta \\theta^2$ to get the correct normalization, \n",
    "- multiply by $-1$, \n",
    "- then take the square root."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read, think about, and execute the following cell\n",
    "\n",
    "sigma_mu = np.diff(np.log(L), n=2)\n",
    "sigma_mu /= (xgrid[1]-xgrid[0])**2\n",
    "sigma_mu *= -1\n",
    "sigma_mu = 1/np.sqrt(sigma_mu)[0]\n",
    "\n",
    "print(\"Fisher matrix error on estimated mean is %.3f\" % sigma_mu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot a Gaussian at the measured $\\mu$ with this error as the scale to see if it matches the numerical likelihood distribution for the three data points above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the following cell\n",
    "\n",
    "xgrid = np.linspace(0.0,2.0,1000)\n",
    "L = np.prod([___,___,___],axis=0) # Total L is ???\n",
    "# complete the following for measured mean and Fisher error\n",
    "Lfit = norm.pdf(xgrid,loc=___[___],scale=___)  \n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 5))\n",
    "\n",
    "# plot numerical likelihood\n",
    "plt.plot(___, ___, ls='-', c='black', \n",
    "         label=r'$L(\\{x\\})$')\n",
    "\n",
    "# plot fitted Gaussian with arbitrary normalizing constant\n",
    "# offset for ease of viewing\n",
    "C = 1.95\n",
    "plt.plot(xgrid, C * Lfit + 1.0, ls='dashed', \n",
    "         c='black', label=r'$L_\\mathrm{fit}(\\{x\\})$')\n",
    "\n",
    "plt.xlim(0.2, 1.8)\n",
    "plt.ylim(0, 8.0)\n",
    "plt.xlabel('$\\mu$') #Leave out or adjust if no latex\n",
    "plt.ylabel(r'$p(x_i|\\mu,\\sigma)$') #Leave out or adjust if no latex\n",
    "plt.title('MLE for Gaussian Distribution')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks pretty, pretty, pretty good. (\"Curb your enthusiasm\" reference to see who is paying attention) \n",
    "\n",
    "But does this agree with the general homoescedastic sample mean uncertainty? <font color='red'>Compute $\\sigma_\\mu$ from one of the formulae above with $N=3$. Does it agree with the Fisher matrix error?</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What is $\\pm \\sigma$? Errors as Gaussian Approximations to the Likelihood Function\n",
    "\n",
    "The result for $\\sigma_{\\mu}$ has been derived by expanding $\\ln L$ in a Taylor series and retaining terms up to second order (essentially, $\\ln L$ is approximated by a parabola, or an ellipsoidal surface in multidimensional cases, around its maximum). If this expansion is exact (as is the case for a Gaussian error distribution), then we've completely captured the error information.\n",
    "\n",
    "In general, this is not the case and the likelihood surface can significantly deviate from a smooth elliptical surface. Furthermore, it often happens in practice that the likelihood surface is multimodal. It is always a good idea to visualize the likelihood surface when in doubt (see examples in §5.6 in the textbook)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What is $\\pm \\sigma$? Errors as Gaussian Approximations to the Likelihood Function\n",
    "\n",
    "The $(\\hat \\mu - \\sigma_\\mu, \\hat \\mu + \\sigma_\\mu)$ range gives us a **confidence interval**.\n",
    "\n",
    "In frequentist interptetation, if we repeated the same measurement a hundred times, we'd find for 68 experiments the true value was within their computed confidence intervals ($1 \\sigma$ errors)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## MLE applied to a Heteroscedastic Gaussian <a class=\"anchor\" id=\"five\"></a>\n",
    "\n",
    "Now let's look a case where the uncertainties are heteroscedastic.  For example if we are measuring the length of a rod and have $N$ measurements, $\\{x_i\\}$, where the uncertainty for each measurement, $\\sigma_i$ is known.  Since $\\sigma$ is not a constant, then following the above, we have\n",
    "\n",
    "$$\\ln L = {\\rm constant} - \\sum_{i=1}^N \\frac{(x_i - \\mu)^2}{2\\sigma_i^2}.$$\n",
    "\n",
    "Taking the derivative:\n",
    "$$\\frac{d\\;{\\rm lnL}(\\mu)}{d\\mu}\\Biggr\\rvert_{\\hat \\mu} = \\sum_{i=1}^N \\frac{(x_i - \\hat \\mu)}{\\sigma_i^2} = 0,$$\n",
    "then simplifying:\n",
    "\n",
    "$$\\sum_{i=1}^N \\frac{x_i}{\\sigma_i^2} = \\sum_{i=1}^N \\frac{\\hat \\mu}{\\sigma_i^2},$$\n",
    "\n",
    "yields a MLE solution of \n",
    "$$\\hat \\mu = \\frac{\\sum_i^N (x_i/\\sigma_i^2)}{\\sum_i^N (1/\\sigma_i^2)},$$\n",
    "\n",
    "with uncertainty\n",
    "$$\\sigma_{\\mu} = \\left( \\sum_{i=1}^N \\frac{1}{\\sigma_i^2}\\right)^{-1/2}.$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Working with non-Gaussian Likelihoods <a class=\"anchor\" id=\"six\"></a>\n",
    "## *NOTE:* Only Graduate Student are required to complete this section\n",
    "As an example of MLE with non-Gaussian probability density we can use the same formalism above for a Poisson distribution. In this case we write the probability disrtibution as\n",
    "\n",
    "$$p(x_i|\\mu) = \\frac{e^{-\\mu}\\mu^{x_i}}{x_i!}$$\n",
    "\n",
    "with $\\mu$ the average number of events, $N$ is the number of observed events, and $\\{x_i\\}$ are the measured data.\n",
    "\n",
    "As we saw before, this distribution is particularly useful for characterizing the number of soldiers in the Prussian army killed accidentally by horse kicks.\n",
    "\n",
    "We can then write the likelihood as\n",
    "\n",
    "$$L \\equiv p(\\{x_i\\}|\\mu) = \\prod_{i=1}^{N} \\frac{e^{-\\mu}\\mu^{x_i}}{x_i!}$$\n",
    "\n",
    "and the $\\ln L$ as\n",
    "\n",
    "$$\\ln L = \\sum_{i=1}^{N} \\ln \\bigg( \\frac{e^{-\\mu}\\mu^{x_i}}{x_i!} \\bigg)$$\n",
    "\n",
    "$$= \\sum_{i=1}^{N} -\\mu + x_i \\; \\ln(\\mu) - \\ln({x_i!})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install seaborn.\n",
    "# This package can make matplotlib prettier\n",
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this cell to plot some Poisson draws from different means\n",
    "from scipy.stats import poisson\n",
    "import seaborn as sns\n",
    "\n",
    "# generate samples for different values of mu\n",
    "kpts=np.arange(0,25)\n",
    "for mu, c in zip([1,3,12], \n",
    "                 sns.color_palette()[:4]):\n",
    "    # random draws\n",
    "    randomVariates = poisson.rvs(mu, size=1000)\n",
    "    # histogram of random draws\n",
    "    plt.hist(randomVariates, density=True, color=c, alpha=0.2, \n",
    "             bins=range(0,26), label='$\\mu=' + np.str(mu) + '$')\n",
    "    # probability density at bin locations\n",
    "    plt.plot(kpts, poisson.pmf(kpts, mu), '.', color=c)\n",
    "    \n",
    "plt.legend()\n",
    "plt.title(\"Poisson Distribution\")\n",
    "plt.xlabel(\"Number of Events\")\n",
    "plt.ylabel(\"Normed Counts\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>Complete and execute the following cell to generate $5$ random samples from a $\\mu=12$ Poisson distribution and find the MLE $\\hat\\mu$ from the data.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poisson_data = poisson.rvs(12, size=5)\n",
    "\n",
    "# Scan across 1000 possible mu values from 3 to 20.\n",
    "mu_proposed = np.linspace(___,___,___)\n",
    "\n",
    "# compute the lnL for each possible mu.\n",
    "lnL_scan = []\n",
    "for mu in mu_proposed:\n",
    "    lnL_temp = poisson.logpmf(poisson_data, mu=___) # gives you the log prob. density; useful!\n",
    "    lnL_temp = ___.___(___) # sum over the log pmf of all data points\n",
    "    lnL_scan.append( lnL_temp )\n",
    "    \n",
    "# convert to numpy array\n",
    "lnL_scan = np.array(lnL_scan)\n",
    "\n",
    "# write some quick code below to find the element of \n",
    "# mu_proposed that maximizes the lnL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>Plot lnL vs mu_proposed and indicate where it is maximized.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximizing the $\\ln L$ \n",
    "\n",
    "For the Poisson distribution we can solve for the maximum liklehood analytically\n",
    "\n",
    "$$\\frac{\\partial \\; L(\\mu)}{\\partial \\; \\mu} = \\frac{\\partial \\; }{\\partial \\; \\mu} \\bigg( \\sum_{i=1}^{N} -\\mu + x_i \\; \\ln(\\mu)\\bigg)$$\n",
    "\n",
    "$$0 = \\sum_{i=1}^{N} \\bigg( -1 + \\frac{x_i}{\\mu} \\bigg)$$\n",
    "$$\\hat\\mu = \\frac{1}{N} \\sum_{i=1}^{N} x_i $$\n",
    "\n",
    "What do you know!? The same as for a homoescedastic Gaussian! \n",
    "\n",
    "***For many likelihoods we cannot solve for the maximum analytically, and we have to resort to numerical solutions.*** We'll treat these in detail later using MCMC and robust statistics that account for outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>Use this formula to compute $\\hat\\mu$ for your `poisson_data` above. Does it agree with your numerical estimate?</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "Today we heard about:\n",
    "\n",
    "* Classical vs. Bayesian Inference (more next week)\n",
    "* Likelihood function\n",
    "* Maximum Likelihood Estimation\n",
    "  * Homoscedastic Samples\n",
    "  * Heteroscedastic Samples\n",
    "* Quantifying uncertainty on ML estimates\n",
    "* Working with non-Gaussian likelihoods"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:astr8070] *",
   "language": "python",
   "name": "conda-env-astr8070-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "livereveal": {
   "scroll": true,
   "start_slideshow_at": "selected",
   "theme": "sky"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
